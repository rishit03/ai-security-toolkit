# ğŸ›¡ï¸ AI Security Toolkit

A red-team framework for testing the vulnerabilities of AI models through adversarial attacks, privacy leakage, and model exploitation techniques â€” built and maintained by [@rishit03](https://github.com/rishit03).

---

## ğŸš€ Features

âœ… 5+ attack modules  
âœ… Unified logging and visualization  
âœ… Command-line interface (interactive menu)  
âœ… Modular, reusable, and pip-installable  
âœ… Built using TensorFlow, CleverHans, and Python's best practices

---

## ğŸ“¦ Modules Included

| Module Name                | Description |
|----------------------------|-------------|
| ğŸ”“ Adversarial Attack (FGSM)     | Confuses the model with small pixel changes |
| ğŸ’‰ Label Flip Poisoning          | Modifies training labels to reduce model accuracy |
| ğŸ§  Membership Inference Attack  | Infers if a data point was used in training |
| ğŸª Model Inversion              | Reconstructs training images from the model |
| ğŸ§¬ Model Stealing               | Clones the target model using black-box queries |
| ğŸ¯ Backdoor Trigger Attack      | Embeds a hidden trigger that forces misclassification |

---

## ğŸ’» CLI Usage

```bash
# After pip install or cloning locally
python ai_toolkit/run.py
